# åˆ†å—ç­–ç•¥
# =================
# Operation classes
# =================
# Question: 
# 1. ä¸åŒçš„ç¼–ç ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒï¼‰æ˜¯å¦éœ€è¦ä¸åŒçš„åˆ†å—ç­–ç•¥ï¼Ÿ
# 2. ä¸åŒæ ¼å¼çš„æ•°æ®ï¼ˆå¦‚PDFã€HTMLï¼‰æ˜¯å¦éœ€è¦ä¸åŒçš„åˆ†å—ç­–ç•¥ï¼Ÿ
#    - ä¸“ç”¨åˆ†å‰²å™¨ - è‡ªé€‚åº”é€‰æ‹©å‡½æ•°ï¼Ÿ
# 3. åˆ†å—çš„å¤§å°å’Œé‡å åº¦å¦‚ä½•å½±å“æ¨¡å‹çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ï¼Ÿ 
#    - https://zilliz.com.cn/learn/langchain-chunking-milvus
# 4. å¦‚ä½•å¤„ç†åˆ†å—åçš„æ•°æ®ï¼Œä»¥ç¡®ä¿ä¿¡æ¯çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼Ÿ
# 5. åˆ†å—ç­–ç•¥æ˜¯å¦éœ€è¦æ ¹æ®æ•°æ®çš„ç‰¹æ€§è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Ÿ
# 6. å¦‚ä½•è¯„ä¼°åˆ†å—ç­–ç•¥çš„æ•ˆæœï¼Œä»¥ä¾¿è¿›è¡Œä¼˜åŒ–å’Œæ”¹è¿›ï¼Ÿ
# =================
# !ATTENTION:
# 1. Tokenizeä¸­æ–‡ã€æ—¥æ–‡å¯èƒ½ä¼šäº§ç”Ÿä¸¤ä¸ªtokenï¼Œåˆ†å—ä¸å½“ä¼šé€ æˆæ­§ä¹‰
# 2. Unicodeçš„ä¸­æ–‡ç¼–ç 
# 3. ä¼ å…¥**kwargs, æ³¨æ„è°ƒç”¨æ–¹å¼
# =================
from typing import Union, List
from langchain_core.documents import Document
import logging

logger = logging.getLogger(__name__)

__OPERATOR__ = {}

def register_operator(name:str):
    def wrapper(cls):
        if name in __OPERATOR__:
            logger.error(f"Operator {name} is already registered.")
            raise ValueError(f"Operator {name} is already registered.")
        __OPERATOR__[name] = cls
        return cls
    return wrapper

def get_operator(name:str):
    if name not in __OPERATOR__:
        logger.error(f"Operator {name} is not registered.")
        raise ValueError(f"Operator {name} is not registered.")
    return __OPERATOR__[name]

# æŠ½è±¡ç±»éœ€è¦å®ç°ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ
class BaseOperator:
    def __init__(self, **kwargs):
        self.kwargs = kwargs
        # å¯èƒ½éœ€è¦åˆå§‹åŒ–ä¸€äº›å‚æ•°æˆ–é…ç½®
    def execute(self, *args, **kwargs):
        raise NotImplementedError("Subclasses should implement this method.")
    # TODO: modelé€‰æ‹©
    def _tokenize_text(self, text: str, model="Qwen/Qwen3-30B-A3B"):
        """ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯"""
        # ä½¿ç”¨Hugging Faceçš„transformersåº“
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model)
        return tokenizer.tokenize(text)

# ================
# LangChainæä¾›çš„åˆ†å—ç­–ç•¥
# ================
from langchain.text_splitter import CharacterTextSplitter
@register_operator("langchain-length-based")
class langchain_length_based_splitter(BaseOperator):
    def __init__(self, separator="", chunk_size=512, chunk_overlap=51, is_tokenized=False, **kwargs):
        self.splitter = CharacterTextSplitter(
            separator=separator,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            **kwargs,
        )
        self.is_tokenized = is_tokenized
    def execute(self, text: str):
        # !!!å¿…é¡»ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²ç±»å‹ã€ç¼–ç æ–¹å¼æ˜¯UTF-8
        if self.is_tokenized:
            text = super()._tokenize_text(text)
            print(text)
        return self.splitter.split_text(text)    

from langchain.text_splitter import RecursiveCharacterTextSplitter
@register_operator("langchain-structure-based")
class langchain_structure_based_splitter(BaseOperator):
    def __init__(self, chunk_size=512, chunk_overlap=51, is_tokenized=False, **kwargs):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            **kwargs,
        )
        self.is_tokenized = is_tokenized
    def execute(self, text: str):
        if self.is_tokenized:
            text = super()._tokenize_text(text)
        return self.splitter.split_text(text)

from langchain.text_splitter import MarkdownHeaderTextSplitter
@register_operator("langchain-markdown")
class langchain_markdown_splitter(BaseOperator):
    """
    ä¸€èˆ¬æƒ…å†µä¸‹, è¿˜éœ€è¦æ¥recursiveCharacterTextSplitter
    """
    def __init__(self, headers_to_split_on=[("#", "Header 1"),("##", "Header 2")], **kwargs):
        self.splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on,
            **kwargs,
        )
    def execute(self, text: str):
        return self.splitter.split_text(text)


import re
@register_operator("markdown-splitter")
class MarkdownSplitter(BaseOperator):
    """
    ğŸš€ğŸš€ğŸš€ è‹¥éœ€è¦æ›´å¤æ‚çš„é€»è¾‘,è¯·ä½¿ç”¨æŠ½è±¡è¯­æ³•æ ‘
    ===== é€»è¾‘ =====
    1. åŸºæœ¬é€»è¾‘
        - #ç¬¦å·åˆ†å—
        - \n ç¬¦å·åˆ†å—
        - åˆ†å‰²çº¿åˆ†å—
        - å¤„ç†æ— ç”¨å­—ç¬¦ä¸² å¦‚\t
    2. ç‰¹æ®Šé€»è¾‘ - éœ€è¦åå¤„ç†
        - è¡Œé—´å…¬å¼
        - è¡Œé—´ä»£ç å—
        - è¡Œå†…å†…å®¹å…¬å¼ç­‰ç­‰

    """
    DEFAULT_HEADER_KEYS = {
        "#": "Header 1",
        "##": "Header 2",
        "###": "Header 3",
        "####": "Header 4",
        "#####": "Header 5",
        "######": "Header 6",
    }

    def __init__(self, 
                 headers_to_split_on: Union[dict[str, str], None] = None,
                 strip_headers: bool = True,  # noqa: FBT001,FBT002
                 **kwargs):
        if headers_to_split_on is None:
            self.headers_to_split_on = self.DEFAULT_HEADER_KEYS
        else:
            self.headers_to_split_on = headers_to_split_on
        
        self.strip_headers = strip_headers
        self.kwargs = kwargs
        self.chunks : list[Document] = []  # å­˜å‚¨åˆ†å—åçš„æ–‡æœ¬
        self.current_chunk = Document(page_content="")  # å½“å‰åˆ†å—å†…å®¹
        self.current_header_stack: list[tuple[int, str]] = []  # å½“å‰æ ‡é¢˜æ ˆ
        
    def execute(self, text: str) -> list[Document]:
        """
        æ‰§è¡Œåˆ†å—æ“ä½œ
        :param text: è¾“å…¥çš„æ–‡æœ¬
        :return: åˆ†å—åçš„æ–‡æœ¬åˆ—è¡¨
        """
        self.chunks = []  # æ¸…ç©ºä¹‹å‰çš„åˆ†å—ç»“æœ
        self.current_chunk = Document(page_content="")  # é‡ç½®å½“å‰åˆ†å—å†…å®¹
        self.current_header_stack = []  # é‡ç½®å½“å‰æ ‡é¢˜æ ˆ

        raw_lines = text.splitlines(keepends=True)
        while raw_lines:
            raw_line = raw_lines.pop(0)
            # ==== ä¼˜å…ˆçº§  =====
            # è¿™äº›matchä¸æ·»åŠ åœ¨chunkä¸­,headerå¯èƒ½é™¤å¤–
            header_match = self._split_by_headers(raw_line)
            math_match = self._split_by_math(raw_line)
            code_match = self._split_by_code(raw_line)
            horz_match = self._split_by_horz(raw_line)
  
            if header_match:
                self._complete_current_chunk()
                depth = len(header_match.group(1))
                header_text = header_match.group(2)
                if not self.strip_headers:
                    # å¦‚æœä¸éœ€è¦å»é™¤æ ‡é¢˜ï¼Œåˆ™å°†æ ‡é¢˜æ·»åŠ åˆ°å½“å‰åˆ†å—
                    self.current_chunk.page_content += f"{header_text}\n"
                
                self._resolve_header_stack(depth, header_text)
            elif math_match:
                # è¡Œé—´å…¬å¼å•ç‹¬åˆ†å—
                self._complete_current_chunk()
                self.current_chunk.page_content = self._aggregate_math_line(
                    self.current_chunk.page_content, raw_lines)
                self.current_chunk.metadata = {"type": "math"}
                self._complete_current_chunk()
            elif code_match:
                # è¡Œé—´ä»£ç å—å•ç‹¬åˆ†å—
                self._complete_current_chunk()
                self.current_chunk.page_content = self._aggregate_code_line(
                    self.current_chunk.page_content, raw_lines)
                self.current_chunk.metadata = {"type": "code"}
                self._complete_current_chunk()
            elif horz_match:
                # åˆ†å‰²çº¿ç›´æ¥åˆ†å—å—ï¼Ÿ
                self._complete_current_chunk()
            else:
                self.current_chunk.page_content += raw_line
        self._complete_current_chunk()
        return self.chunks
    
    def _resolve_header_stack(self, depth: int, header_text: str):
        """
        å¤„ç†æ ‡é¢˜æ ˆï¼Œæ›´æ–°å½“å‰æ ‡é¢˜æ ˆ
        :param depth: æ ‡é¢˜æ·±åº¦
        :param header_text: æ ‡é¢˜æ–‡æœ¬
        """
        # å¦‚æœå½“å‰æ ‡é¢˜æ ˆä¸ä¸ºç©ºä¸”æ·±åº¦å°äºç­‰äºæ ˆé¡¶å…ƒç´ çš„æ·±åº¦ï¼Œåˆ™å¼¹å‡ºæ ˆé¡¶å…ƒç´ 
        while self.current_header_stack and self.current_header_stack[-1][0] >= depth:
            self.current_header_stack.pop()
        
        # æ·»åŠ æ–°çš„æ ‡é¢˜åˆ°å½“å‰æ ‡é¢˜æ ˆ
        self.current_header_stack.append((depth, header_text))
    
    def _complete_current_chunk(self):
        """
        å®Œæˆå½“å‰åˆ†å—ï¼Œå°†å…¶æ·»åŠ åˆ°åˆ†å—åˆ—è¡¨ä¸­
        æ·»åŠ metadataä¿¡æ¯
        """
        chunk = self.current_chunk
        if chunk.page_content and not chunk.page_content.isspace():
            for depth, header in self.current_header_stack:
                key = self.headers_to_split_on.get("#" * depth)
                if key:
                    chunk.metadata[key] = header
            self.chunks.append(chunk)
        # reset current chunk
        # current_chunk åªæ¶‰åŠpage_content
        self.current_chunk = Document(page_content="")

    def _aggregate_code_line(self, current_chunk: str, raw_lines: List[str]):
        chunk = current_chunk 
        while raw_lines:
            raw_line = raw_lines.pop(0)
            if self._split_by_code(raw_line):
                # è¡Œé—´ä»£ç å—
                return chunk
            chunk += raw_line

        logger.warning("No match code found in the remaining lines.")
        return ""

    def _aggregate_math_line(self, current_chunk: str, raw_lines: List[str]):
        chunk = current_chunk 
        while raw_lines:
            raw_line = raw_lines.pop(0)
            if self._split_by_math(raw_line, is_tail=True):
                # è¡Œé—´å…¬å¼
                return chunk
            chunk += raw_line

        logger.warning("No match math found in the remaining lines.")
        return ""
    
    def _split_by_headers(self, line: str):
        match = re.match(r"^(#{1,6}) (.*)", line)
        if match and match.group(1) in self.headers_to_split_on:
            return match 
        return None

    def _split_by_code(self, line: str):
        match = [re.match(rule, line) for rule in [r"^```(.*)", r"^~~~(.*)"]]
        return next((match for match in match if match), None)

    def _split_by_horz(self, line: str):
        match = [re.match(rule, line) for rule in [r"^\*\*\*+\n", r"^---+\n", r"^___+\n"]]
        return next((match for match in match if match), None)

    def _split_by_math(self, line: str, is_tail: bool = False):
        if is_tail:
            match = [re.match(rule, line) for rule in [r"^\$\$(.*)", r"^```(.*)"]]
        else:
            match = [re.match(rule, line) for rule in [r"^\$\$(.*)", r"^```math(.*)"]]
        return next((match for match in match if match), None)

    def _split_by_inline_content(self, text: str):
        """
        å¤„ç†è¡Œå†…å†…å®¹, ä¹Ÿä¼šå‡ºç°è¯¯æŠ¥
        """
        # 1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¡Œå†…å…¬å¼
        inline_math_pattern = r"\$.*?\$"
        text = re.sub(inline_math_pattern, self._post_process_inline_math, text)

        # 2. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¡Œå†…ä»£ç å—
        inline_code_pattern = r"`.*?`"
        text = re.sub(inline_code_pattern, self._post_process_inline_code, text)

        return text
    
    def _post_process_math(self, match: re.Match):
        """
        å¤„ç†è¡Œé—´å…¬å¼
        """
        matched = match.group(0)
        matched = "www.math.com"  # TODO: å¤„ç†è¡Œé—´å…¬å¼
        return matched

    def _post_process_code(self, match: re.Match):
        """
        å¤„ç†è¡Œé—´ä»£ç å—
        """
        matched = match.group(0)
        matched = "www.code.com"  # TODO: å¤„ç†è¡Œé—´ä»£ç å—
        return matched

    def _post_process_inline_math(self, match: re.Match):
        """
        å¤„ç†è¡Œå†…å…¬å¼
        """
        matched = match.group(0)
        matched = "www.inline-math.com"  # TODO: å¤„ç†è¡Œå†…å…¬å¼
        return matched
    
    def _post_process_inline_code(self, match: re.Match):
        """
        å¤„ç†è¡Œå†…ä»£ç å—
        """
        matched = match.group(0)
        matched = "www.inline-code.com"  # TODO: å¤„ç†è¡Œå†…ä»£ç å—
        return matched

if __name__ == "__main__":
    # æµ‹è¯• langchain_markdown_splitter
    splitter = get_operator("markdown-splitter")()
    with open("readme.md", "r", encoding="utf-8") as f:
        text = f.read()
    chunks = splitter.execute(text)
    print(chunks)  # è¾“å‡ºåˆ†å—ç»“æœ


